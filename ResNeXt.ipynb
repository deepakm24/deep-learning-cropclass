{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0b8af5331f5449bc834b21ba7de4b907": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_75b5aa06ce4d4544a5277627861dbf8e",
              "IPY_MODEL_bd52d90f086943b9a6fbf7e28bda3b2e",
              "IPY_MODEL_666eaf054fd146318ad8b2e3c3551863"
            ],
            "layout": "IPY_MODEL_bcb3aa092e86443abaec1ca45b1d49b3"
          }
        },
        "75b5aa06ce4d4544a5277627861dbf8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7843abbd5aca4a198107de5d9d88a513",
            "placeholder": "​",
            "style": "IPY_MODEL_9810364bfe5d46fe9906e510cba128f0",
            "value": "100%"
          }
        },
        "bd52d90f086943b9a6fbf7e28bda3b2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_575e35857f0c4d46922850a8c79aed6a",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ef6bb762c8364774a038ffdf8df8c65a",
            "value": 170498071
          }
        },
        "666eaf054fd146318ad8b2e3c3551863": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_67ea849ddd7248ceb3104c0564c65522",
            "placeholder": "​",
            "style": "IPY_MODEL_e550da425b2a47fbb572e6a588bca9e6",
            "value": " 170498071/170498071 [00:04&lt;00:00, 40179852.69it/s]"
          }
        },
        "bcb3aa092e86443abaec1ca45b1d49b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7843abbd5aca4a198107de5d9d88a513": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9810364bfe5d46fe9906e510cba128f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "575e35857f0c4d46922850a8c79aed6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef6bb762c8364774a038ffdf8df8c65a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "67ea849ddd7248ceb3104c0564c65522": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e550da425b2a47fbb572e6a588bca9e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "0IaDTdxReUDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ConvBlock\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, groups=1, bias=False):\n",
        "        super().__init__()\n",
        "        self.c = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=bias, groups=groups)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.bn(self.c(x))\n",
        "\n",
        "# Bottleneck ResidualBlock \n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride, first=False, cardinatlity=32):\n",
        "        super().__init__()\n",
        "        self.C = cardinatlity\n",
        "        self.downsample = stride==2 or first\n",
        "        res_channels = out_channels // 2\n",
        "        self.c1 = ConvBlock(in_channels, res_channels, 1, 1, 0)\n",
        "        self.c2 = ConvBlock(res_channels, res_channels, 3, stride, 1, self.C)\n",
        "        self.c3 = ConvBlock(res_channels, out_channels, 1, 1, 0)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        if self.downsample:\n",
        "            self.p = ConvBlock(in_channels, out_channels, 1, stride, 0)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        f = self.relu(self.c1(x))\n",
        "        f = self.relu(self.c2(f))\n",
        "        f = self.c3(f)\n",
        "\n",
        "        if self.downsample:\n",
        "            x = self.p(x)\n",
        "\n",
        "        h = self.relu(torch.add(f,x))\n",
        "\n",
        "        return h\n",
        "\n",
        "# ResNeXt\n",
        "class ResNeXt(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        config_name : int, \n",
        "        in_channels : int = 3, \n",
        "        classes : int = 1000,\n",
        "        C : int = 32 # cardinality\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        configurations = {\n",
        "            50 : [3, 4, 6, 3],\n",
        "            101 : [3, 4, 23, 3],\n",
        "            152 : [3, 8, 36, 3]\n",
        "        }\n",
        "\n",
        "        no_blocks = configurations[config_name]\n",
        "\n",
        "        out_features = [256, 512, 1024, 2048]\n",
        "        self.blocks = nn.ModuleList([ResidualBlock(64, 256, 1, True, cardinatlity=32)])\n",
        "\n",
        "        for i in range(len(out_features)):\n",
        "            if i > 0:\n",
        "                self.blocks.append(ResidualBlock(out_features[i-1], out_features[i], 2, cardinatlity=C))\n",
        "            for _ in range(no_blocks[i]-1):\n",
        "                self.blocks.append(ResidualBlock(out_features[i], out_features[i], 1, cardinatlity=C))\n",
        "        \n",
        "        self.conv1 = ConvBlock(in_channels, 64, 7, 2, 3)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc = nn.Linear(2048, classes)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.init_weight()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.maxpool(x)\n",
        "        for block in self.blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def init_weight(self):\n",
        "        for layer in self.modules():\n",
        "            if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
        "                nn.init.kaiming_normal_(layer.weight)"
      ],
      "metadata": {
        "id": "3kbLQQdFea61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    config_name = 50\n",
        "    resnext50 = ResNeXt(config_name, in_channels=3,  C=32)\n",
        "    image = torch.rand(1, 3, 224, 224)\n",
        "    print(resnext50(image).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tn-qR3BifALS",
        "outputId": "3d7f82f6-fb7a-4457-830b-60dadb8bcc94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 1000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "model = ResNeXt(config_name, in_channels=3,  C=32)\n",
        "summary(model,(3,224,224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYznYKfufPoG",
        "outputId": "c569921d-3e37-4799-d9d8-c7eb806eeb47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "         ConvBlock-3         [-1, 64, 112, 112]               0\n",
            "              ReLU-4         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-5           [-1, 64, 56, 56]               0\n",
            "            Conv2d-6          [-1, 128, 56, 56]           8,192\n",
            "       BatchNorm2d-7          [-1, 128, 56, 56]             256\n",
            "         ConvBlock-8          [-1, 128, 56, 56]               0\n",
            "              ReLU-9          [-1, 128, 56, 56]               0\n",
            "           Conv2d-10          [-1, 128, 56, 56]           4,608\n",
            "      BatchNorm2d-11          [-1, 128, 56, 56]             256\n",
            "        ConvBlock-12          [-1, 128, 56, 56]               0\n",
            "             ReLU-13          [-1, 128, 56, 56]               0\n",
            "           Conv2d-14          [-1, 256, 56, 56]          32,768\n",
            "      BatchNorm2d-15          [-1, 256, 56, 56]             512\n",
            "        ConvBlock-16          [-1, 256, 56, 56]               0\n",
            "           Conv2d-17          [-1, 256, 56, 56]          16,384\n",
            "      BatchNorm2d-18          [-1, 256, 56, 56]             512\n",
            "        ConvBlock-19          [-1, 256, 56, 56]               0\n",
            "             ReLU-20          [-1, 256, 56, 56]               0\n",
            "    ResidualBlock-21          [-1, 256, 56, 56]               0\n",
            "           Conv2d-22          [-1, 128, 56, 56]          32,768\n",
            "      BatchNorm2d-23          [-1, 128, 56, 56]             256\n",
            "        ConvBlock-24          [-1, 128, 56, 56]               0\n",
            "             ReLU-25          [-1, 128, 56, 56]               0\n",
            "           Conv2d-26          [-1, 128, 56, 56]           4,608\n",
            "      BatchNorm2d-27          [-1, 128, 56, 56]             256\n",
            "        ConvBlock-28          [-1, 128, 56, 56]               0\n",
            "             ReLU-29          [-1, 128, 56, 56]               0\n",
            "           Conv2d-30          [-1, 256, 56, 56]          32,768\n",
            "      BatchNorm2d-31          [-1, 256, 56, 56]             512\n",
            "        ConvBlock-32          [-1, 256, 56, 56]               0\n",
            "             ReLU-33          [-1, 256, 56, 56]               0\n",
            "    ResidualBlock-34          [-1, 256, 56, 56]               0\n",
            "           Conv2d-35          [-1, 128, 56, 56]          32,768\n",
            "      BatchNorm2d-36          [-1, 128, 56, 56]             256\n",
            "        ConvBlock-37          [-1, 128, 56, 56]               0\n",
            "             ReLU-38          [-1, 128, 56, 56]               0\n",
            "           Conv2d-39          [-1, 128, 56, 56]           4,608\n",
            "      BatchNorm2d-40          [-1, 128, 56, 56]             256\n",
            "        ConvBlock-41          [-1, 128, 56, 56]               0\n",
            "             ReLU-42          [-1, 128, 56, 56]               0\n",
            "           Conv2d-43          [-1, 256, 56, 56]          32,768\n",
            "      BatchNorm2d-44          [-1, 256, 56, 56]             512\n",
            "        ConvBlock-45          [-1, 256, 56, 56]               0\n",
            "             ReLU-46          [-1, 256, 56, 56]               0\n",
            "    ResidualBlock-47          [-1, 256, 56, 56]               0\n",
            "           Conv2d-48          [-1, 256, 56, 56]          65,536\n",
            "      BatchNorm2d-49          [-1, 256, 56, 56]             512\n",
            "        ConvBlock-50          [-1, 256, 56, 56]               0\n",
            "             ReLU-51          [-1, 256, 56, 56]               0\n",
            "           Conv2d-52          [-1, 256, 28, 28]          18,432\n",
            "      BatchNorm2d-53          [-1, 256, 28, 28]             512\n",
            "        ConvBlock-54          [-1, 256, 28, 28]               0\n",
            "             ReLU-55          [-1, 256, 28, 28]               0\n",
            "           Conv2d-56          [-1, 512, 28, 28]         131,072\n",
            "      BatchNorm2d-57          [-1, 512, 28, 28]           1,024\n",
            "        ConvBlock-58          [-1, 512, 28, 28]               0\n",
            "           Conv2d-59          [-1, 512, 28, 28]         131,072\n",
            "      BatchNorm2d-60          [-1, 512, 28, 28]           1,024\n",
            "        ConvBlock-61          [-1, 512, 28, 28]               0\n",
            "             ReLU-62          [-1, 512, 28, 28]               0\n",
            "    ResidualBlock-63          [-1, 512, 28, 28]               0\n",
            "           Conv2d-64          [-1, 256, 28, 28]         131,072\n",
            "      BatchNorm2d-65          [-1, 256, 28, 28]             512\n",
            "        ConvBlock-66          [-1, 256, 28, 28]               0\n",
            "             ReLU-67          [-1, 256, 28, 28]               0\n",
            "           Conv2d-68          [-1, 256, 28, 28]          18,432\n",
            "      BatchNorm2d-69          [-1, 256, 28, 28]             512\n",
            "        ConvBlock-70          [-1, 256, 28, 28]               0\n",
            "             ReLU-71          [-1, 256, 28, 28]               0\n",
            "           Conv2d-72          [-1, 512, 28, 28]         131,072\n",
            "      BatchNorm2d-73          [-1, 512, 28, 28]           1,024\n",
            "        ConvBlock-74          [-1, 512, 28, 28]               0\n",
            "             ReLU-75          [-1, 512, 28, 28]               0\n",
            "    ResidualBlock-76          [-1, 512, 28, 28]               0\n",
            "           Conv2d-77          [-1, 256, 28, 28]         131,072\n",
            "      BatchNorm2d-78          [-1, 256, 28, 28]             512\n",
            "        ConvBlock-79          [-1, 256, 28, 28]               0\n",
            "             ReLU-80          [-1, 256, 28, 28]               0\n",
            "           Conv2d-81          [-1, 256, 28, 28]          18,432\n",
            "      BatchNorm2d-82          [-1, 256, 28, 28]             512\n",
            "        ConvBlock-83          [-1, 256, 28, 28]               0\n",
            "             ReLU-84          [-1, 256, 28, 28]               0\n",
            "           Conv2d-85          [-1, 512, 28, 28]         131,072\n",
            "      BatchNorm2d-86          [-1, 512, 28, 28]           1,024\n",
            "        ConvBlock-87          [-1, 512, 28, 28]               0\n",
            "             ReLU-88          [-1, 512, 28, 28]               0\n",
            "    ResidualBlock-89          [-1, 512, 28, 28]               0\n",
            "           Conv2d-90          [-1, 256, 28, 28]         131,072\n",
            "      BatchNorm2d-91          [-1, 256, 28, 28]             512\n",
            "        ConvBlock-92          [-1, 256, 28, 28]               0\n",
            "             ReLU-93          [-1, 256, 28, 28]               0\n",
            "           Conv2d-94          [-1, 256, 28, 28]          18,432\n",
            "      BatchNorm2d-95          [-1, 256, 28, 28]             512\n",
            "        ConvBlock-96          [-1, 256, 28, 28]               0\n",
            "             ReLU-97          [-1, 256, 28, 28]               0\n",
            "           Conv2d-98          [-1, 512, 28, 28]         131,072\n",
            "      BatchNorm2d-99          [-1, 512, 28, 28]           1,024\n",
            "       ConvBlock-100          [-1, 512, 28, 28]               0\n",
            "            ReLU-101          [-1, 512, 28, 28]               0\n",
            "   ResidualBlock-102          [-1, 512, 28, 28]               0\n",
            "          Conv2d-103          [-1, 512, 28, 28]         262,144\n",
            "     BatchNorm2d-104          [-1, 512, 28, 28]           1,024\n",
            "       ConvBlock-105          [-1, 512, 28, 28]               0\n",
            "            ReLU-106          [-1, 512, 28, 28]               0\n",
            "          Conv2d-107          [-1, 512, 14, 14]          73,728\n",
            "     BatchNorm2d-108          [-1, 512, 14, 14]           1,024\n",
            "       ConvBlock-109          [-1, 512, 14, 14]               0\n",
            "            ReLU-110          [-1, 512, 14, 14]               0\n",
            "          Conv2d-111         [-1, 1024, 14, 14]         524,288\n",
            "     BatchNorm2d-112         [-1, 1024, 14, 14]           2,048\n",
            "       ConvBlock-113         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-114         [-1, 1024, 14, 14]         524,288\n",
            "     BatchNorm2d-115         [-1, 1024, 14, 14]           2,048\n",
            "       ConvBlock-116         [-1, 1024, 14, 14]               0\n",
            "            ReLU-117         [-1, 1024, 14, 14]               0\n",
            "   ResidualBlock-118         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-119          [-1, 512, 14, 14]         524,288\n",
            "     BatchNorm2d-120          [-1, 512, 14, 14]           1,024\n",
            "       ConvBlock-121          [-1, 512, 14, 14]               0\n",
            "            ReLU-122          [-1, 512, 14, 14]               0\n",
            "          Conv2d-123          [-1, 512, 14, 14]          73,728\n",
            "     BatchNorm2d-124          [-1, 512, 14, 14]           1,024\n",
            "       ConvBlock-125          [-1, 512, 14, 14]               0\n",
            "            ReLU-126          [-1, 512, 14, 14]               0\n",
            "          Conv2d-127         [-1, 1024, 14, 14]         524,288\n",
            "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
            "       ConvBlock-129         [-1, 1024, 14, 14]               0\n",
            "            ReLU-130         [-1, 1024, 14, 14]               0\n",
            "   ResidualBlock-131         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-132          [-1, 512, 14, 14]         524,288\n",
            "     BatchNorm2d-133          [-1, 512, 14, 14]           1,024\n",
            "       ConvBlock-134          [-1, 512, 14, 14]               0\n",
            "            ReLU-135          [-1, 512, 14, 14]               0\n",
            "          Conv2d-136          [-1, 512, 14, 14]          73,728\n",
            "     BatchNorm2d-137          [-1, 512, 14, 14]           1,024\n",
            "       ConvBlock-138          [-1, 512, 14, 14]               0\n",
            "            ReLU-139          [-1, 512, 14, 14]               0\n",
            "          Conv2d-140         [-1, 1024, 14, 14]         524,288\n",
            "     BatchNorm2d-141         [-1, 1024, 14, 14]           2,048\n",
            "       ConvBlock-142         [-1, 1024, 14, 14]               0\n",
            "            ReLU-143         [-1, 1024, 14, 14]               0\n",
            "   ResidualBlock-144         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-145          [-1, 512, 14, 14]         524,288\n",
            "     BatchNorm2d-146          [-1, 512, 14, 14]           1,024\n",
            "       ConvBlock-147          [-1, 512, 14, 14]               0\n",
            "            ReLU-148          [-1, 512, 14, 14]               0\n",
            "          Conv2d-149          [-1, 512, 14, 14]          73,728\n",
            "     BatchNorm2d-150          [-1, 512, 14, 14]           1,024\n",
            "       ConvBlock-151          [-1, 512, 14, 14]               0\n",
            "            ReLU-152          [-1, 512, 14, 14]               0\n",
            "          Conv2d-153         [-1, 1024, 14, 14]         524,288\n",
            "     BatchNorm2d-154         [-1, 1024, 14, 14]           2,048\n",
            "       ConvBlock-155         [-1, 1024, 14, 14]               0\n",
            "            ReLU-156         [-1, 1024, 14, 14]               0\n",
            "   ResidualBlock-157         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-158          [-1, 512, 14, 14]         524,288\n",
            "     BatchNorm2d-159          [-1, 512, 14, 14]           1,024\n",
            "       ConvBlock-160          [-1, 512, 14, 14]               0\n",
            "            ReLU-161          [-1, 512, 14, 14]               0\n",
            "          Conv2d-162          [-1, 512, 14, 14]          73,728\n",
            "     BatchNorm2d-163          [-1, 512, 14, 14]           1,024\n",
            "       ConvBlock-164          [-1, 512, 14, 14]               0\n",
            "            ReLU-165          [-1, 512, 14, 14]               0\n",
            "          Conv2d-166         [-1, 1024, 14, 14]         524,288\n",
            "     BatchNorm2d-167         [-1, 1024, 14, 14]           2,048\n",
            "       ConvBlock-168         [-1, 1024, 14, 14]               0\n",
            "            ReLU-169         [-1, 1024, 14, 14]               0\n",
            "   ResidualBlock-170         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-171          [-1, 512, 14, 14]         524,288\n",
            "     BatchNorm2d-172          [-1, 512, 14, 14]           1,024\n",
            "       ConvBlock-173          [-1, 512, 14, 14]               0\n",
            "            ReLU-174          [-1, 512, 14, 14]               0\n",
            "          Conv2d-175          [-1, 512, 14, 14]          73,728\n",
            "     BatchNorm2d-176          [-1, 512, 14, 14]           1,024\n",
            "       ConvBlock-177          [-1, 512, 14, 14]               0\n",
            "            ReLU-178          [-1, 512, 14, 14]               0\n",
            "          Conv2d-179         [-1, 1024, 14, 14]         524,288\n",
            "     BatchNorm2d-180         [-1, 1024, 14, 14]           2,048\n",
            "       ConvBlock-181         [-1, 1024, 14, 14]               0\n",
            "            ReLU-182         [-1, 1024, 14, 14]               0\n",
            "   ResidualBlock-183         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-184         [-1, 1024, 14, 14]       1,048,576\n",
            "     BatchNorm2d-185         [-1, 1024, 14, 14]           2,048\n",
            "       ConvBlock-186         [-1, 1024, 14, 14]               0\n",
            "            ReLU-187         [-1, 1024, 14, 14]               0\n",
            "          Conv2d-188           [-1, 1024, 7, 7]         294,912\n",
            "     BatchNorm2d-189           [-1, 1024, 7, 7]           2,048\n",
            "       ConvBlock-190           [-1, 1024, 7, 7]               0\n",
            "            ReLU-191           [-1, 1024, 7, 7]               0\n",
            "          Conv2d-192           [-1, 2048, 7, 7]       2,097,152\n",
            "     BatchNorm2d-193           [-1, 2048, 7, 7]           4,096\n",
            "       ConvBlock-194           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-195           [-1, 2048, 7, 7]       2,097,152\n",
            "     BatchNorm2d-196           [-1, 2048, 7, 7]           4,096\n",
            "       ConvBlock-197           [-1, 2048, 7, 7]               0\n",
            "            ReLU-198           [-1, 2048, 7, 7]               0\n",
            "   ResidualBlock-199           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-200           [-1, 1024, 7, 7]       2,097,152\n",
            "     BatchNorm2d-201           [-1, 1024, 7, 7]           2,048\n",
            "       ConvBlock-202           [-1, 1024, 7, 7]               0\n",
            "            ReLU-203           [-1, 1024, 7, 7]               0\n",
            "          Conv2d-204           [-1, 1024, 7, 7]         294,912\n",
            "     BatchNorm2d-205           [-1, 1024, 7, 7]           2,048\n",
            "       ConvBlock-206           [-1, 1024, 7, 7]               0\n",
            "            ReLU-207           [-1, 1024, 7, 7]               0\n",
            "          Conv2d-208           [-1, 2048, 7, 7]       2,097,152\n",
            "     BatchNorm2d-209           [-1, 2048, 7, 7]           4,096\n",
            "       ConvBlock-210           [-1, 2048, 7, 7]               0\n",
            "            ReLU-211           [-1, 2048, 7, 7]               0\n",
            "   ResidualBlock-212           [-1, 2048, 7, 7]               0\n",
            "          Conv2d-213           [-1, 1024, 7, 7]       2,097,152\n",
            "     BatchNorm2d-214           [-1, 1024, 7, 7]           2,048\n",
            "       ConvBlock-215           [-1, 1024, 7, 7]               0\n",
            "            ReLU-216           [-1, 1024, 7, 7]               0\n",
            "          Conv2d-217           [-1, 1024, 7, 7]         294,912\n",
            "     BatchNorm2d-218           [-1, 1024, 7, 7]           2,048\n",
            "       ConvBlock-219           [-1, 1024, 7, 7]               0\n",
            "            ReLU-220           [-1, 1024, 7, 7]               0\n",
            "          Conv2d-221           [-1, 2048, 7, 7]       2,097,152\n",
            "     BatchNorm2d-222           [-1, 2048, 7, 7]           4,096\n",
            "       ConvBlock-223           [-1, 2048, 7, 7]               0\n",
            "            ReLU-224           [-1, 2048, 7, 7]               0\n",
            "   ResidualBlock-225           [-1, 2048, 7, 7]               0\n",
            "AdaptiveAvgPool2d-226           [-1, 2048, 1, 1]               0\n",
            "          Linear-227                 [-1, 1000]       2,049,000\n",
            "================================================================\n",
            "Total params: 25,028,904\n",
            "Trainable params: 25,028,904\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 471.65\n",
            "Params size (MB): 95.48\n",
            "Estimated Total Size (MB): 567.70\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in relevant libraries, and alias where appropriate\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define relevant variables for the ML task\n",
        "batch_size = 64\n",
        "num_classes = 10\n",
        "learning_rate = 0.001\n",
        "num_epochs = 1\n",
        "\n",
        "# Device will determine whether to run the training on GPU or CPU.\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            \n",
        "            \n"
      ],
      "metadata": {
        "id": "Fj9Lfh6pQoEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use transforms.compose method to reformat images for modeling,\n",
        "# and save to variable all_transforms for later use\n",
        "all_transforms = transforms.Compose([transforms.Resize((32,32)),\n",
        "                                     transforms.ToTensor(),\n",
        "                                     transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
        "                                                          std=[0.2023, 0.1994, 0.2010])\n",
        "                                     ])\n",
        "# Create Training dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root = './data',\n",
        "                                             train = True,\n",
        "                                             transform = all_transforms,\n",
        "                                             download = True)\n",
        "\n",
        "# Create Testing dataset\n",
        "test_dataset = torchvision.datasets.CIFAR10(root = './data',\n",
        "                                            train = False,\n",
        "                                            transform = all_transforms,\n",
        "                                            download=True)\n",
        "\n",
        "# Instantiate loader objects to facilitate processing\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "                                           batch_size = batch_size,\n",
        "                                           shuffle = True)\n",
        "\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "                                           batch_size = batch_size,\n",
        "                                           shuffle = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "0b8af5331f5449bc834b21ba7de4b907",
            "75b5aa06ce4d4544a5277627861dbf8e",
            "bd52d90f086943b9a6fbf7e28bda3b2e",
            "666eaf054fd146318ad8b2e3c3551863",
            "bcb3aa092e86443abaec1ca45b1d49b3",
            "7843abbd5aca4a198107de5d9d88a513",
            "9810364bfe5d46fe9906e510cba128f0",
            "575e35857f0c4d46922850a8c79aed6a",
            "ef6bb762c8364774a038ffdf8df8c65a",
            "67ea849ddd7248ceb3104c0564c65522",
            "e550da425b2a47fbb572e6a588bca9e6"
          ]
        },
        "id": "6T9CCQyaQ6aH",
        "outputId": "0e0bb31e-faa8-4676-a41d-a92ca1322851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b8af5331f5449bc834b21ba7de4b907"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNeXt(config_name, in_channels=3,  C=32)\n",
        "# Set Loss function with criterion\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Set optimizer with optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)  \n",
        "\n",
        "total_step = len(train_loader)"
      ],
      "metadata": {
        "id": "A_40T476RL8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We use the pre-defined number of epochs to determine how many iterations to train the network on\n",
        "for epoch in range(num_epochs):\n",
        "\t#Load in the data in batches using the train_loader object\n",
        "    for i, (images, labels) in enumerate(train_loader):  \n",
        "        # Move tensors to the configured device\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        print( type(images))\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        #print( type(outputs))\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "\n",
        "            \n",
        "            \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "-_7Ao2v9ReBE",
        "outputId": "e49fea4a-67fc-45cd-f0c4-bfa8a691aefc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n",
            "<class 'torch.Tensor'>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-5d652290d89f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    print('Accuracy of the network on the {} train images: {} %'.format(50000, 100 * correct / total))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Je_4g6FAIECY",
        "outputId": "2cc5d43e-7063-49a2-d0a8-05d5ce5a9e3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 50000 train images: 32.08 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "d0b3jOchIDQy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRE TRAINED"
      ],
      "metadata": {
        "id": "woTLlml9Q6z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnext50_32x4d', pretrained=False)\n",
        "# or\n",
        "# model = torch.hub.load('pytorch/vision:v0.10.0', 'resnext101_32x8d', pretrained=True)\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XmDEhG8MsO-",
        "outputId": "a65767dc-5503-4f15-fecb-8f673f81aef1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
              "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
              "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download an example image from the pytorch website\n",
        "import urllib\n",
        "url, filename = (\"https://github.com/pytorch/hub/raw/master/images/dog.jpg\", \"dog.jpg\")\n",
        "try: urllib.URLopener().retrieve(url, filename)\n",
        "except: urllib.request.urlretrieve(url, filename)"
      ],
      "metadata": {
        "id": "xBqrtSPINozt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample execution (requires torchvision)\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "input_image = Image.open(filename)\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "input_tensor = preprocess(input_image)\n",
        "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
        "\n",
        "# move the input and model to GPU for speed if available\n",
        "if torch.cuda.is_available():\n",
        "    input_batch = input_batch.to('cuda')\n",
        "    model.to('cuda')\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(input_batch)\n",
        "# Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n",
        "print(output[0])\n",
        "# The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
        "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "print(probabilities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CliWPjApNzy9",
        "outputId": "031be4ca-8f11-471b-b156-dabb0ae481ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 3.0500e-02,  3.1743e-02, -9.1938e-02, -2.5634e-02,  2.3962e-02,\n",
            "         5.3728e-02,  5.2659e-02, -8.2870e-02, -5.7828e-02,  1.3990e-02,\n",
            "        -8.4887e-02,  5.8445e-03, -1.6112e-03, -2.6468e-02,  2.8511e-02,\n",
            "        -6.7608e-02, -4.3591e-03, -6.0871e-04,  4.7630e-02, -2.6882e-03,\n",
            "         7.1166e-02, -4.7646e-02,  3.1746e-02, -1.7107e-03,  4.4091e-02,\n",
            "         6.3293e-02, -4.8761e-02,  3.3195e-02,  6.4616e-03, -5.4699e-03,\n",
            "         1.9934e-02,  5.0098e-02, -7.7718e-02, -3.4700e-02,  5.3745e-02,\n",
            "         8.4088e-03, -7.3811e-02, -4.2360e-02, -7.2873e-02, -1.1162e-02,\n",
            "         1.1549e-02, -3.6094e-03,  3.9634e-02, -6.2881e-02, -7.8493e-02,\n",
            "         8.9330e-02, -5.2243e-02, -3.2481e-02, -6.3031e-02, -4.8969e-02,\n",
            "         5.5314e-02,  1.0301e-02, -2.0942e-02, -3.2314e-02,  6.0259e-02,\n",
            "         1.8100e-02,  3.6978e-02, -4.7361e-02, -9.1287e-02, -1.7217e-02,\n",
            "        -9.3702e-03, -1.1034e-03, -4.4602e-04,  9.5655e-03, -1.0794e-01,\n",
            "         4.2220e-02,  5.4078e-02, -3.8995e-02,  1.2155e-03, -1.5582e-02,\n",
            "         4.2489e-02, -8.9810e-02,  6.4015e-02, -2.1373e-02,  2.4855e-02,\n",
            "        -1.3144e-03, -2.6281e-02,  6.7014e-02, -1.2254e-02, -3.0614e-02,\n",
            "         2.9271e-03,  1.0895e-02,  5.1118e-02,  6.2700e-02,  5.0791e-03,\n",
            "         5.6007e-03, -2.3549e-02, -7.8084e-02,  2.9054e-02, -5.4543e-02,\n",
            "        -9.4332e-02,  1.3269e-01,  1.2491e-02,  3.1077e-02,  4.2319e-02,\n",
            "        -5.9262e-03, -6.4064e-02,  1.0815e-01,  1.0809e-02,  7.1444e-02,\n",
            "         1.4300e-03,  4.8371e-03,  2.2787e-02, -7.5338e-02,  4.6929e-03,\n",
            "        -4.9486e-03,  5.4793e-02, -1.1692e-01,  2.0656e-02,  2.6862e-02,\n",
            "        -7.6426e-02,  1.3033e-01,  2.5752e-02,  3.0014e-03,  4.3109e-02,\n",
            "         1.2226e-02, -7.2741e-02, -1.9493e-02,  1.8463e-02,  1.4445e-02,\n",
            "        -1.7389e-02, -1.9664e-02, -4.9227e-03, -6.6875e-02,  1.0136e-01,\n",
            "        -1.8705e-02,  4.7230e-02,  7.1190e-02, -1.3256e-02, -6.1259e-02,\n",
            "        -1.7233e-02,  8.9582e-02,  1.6247e-02, -3.3230e-02,  9.6286e-04,\n",
            "        -4.3854e-02,  5.1469e-02,  9.2601e-03, -2.0591e-04, -7.1918e-02,\n",
            "         6.0009e-02, -5.7920e-02, -3.1317e-03, -1.9036e-02, -8.7580e-03,\n",
            "        -1.4241e-02, -4.3060e-02, -8.3699e-02,  7.1533e-02,  1.5187e-02,\n",
            "        -1.1191e-01, -1.0165e-02,  2.4806e-02, -8.0729e-02, -5.4204e-02,\n",
            "        -6.1989e-02, -6.5866e-02, -7.5142e-03,  6.7895e-02, -1.1325e-01,\n",
            "        -4.4834e-02,  1.6660e-02, -1.2921e-01, -4.1855e-02, -3.3801e-02,\n",
            "        -1.8640e-02,  4.2277e-02,  2.7276e-02,  3.8276e-02,  1.8047e-02,\n",
            "        -3.2613e-02,  5.9252e-03, -4.9923e-02, -1.9763e-02, -5.7685e-02,\n",
            "         7.1115e-02,  2.1328e-02,  3.0422e-02, -1.4609e-02,  4.6763e-02,\n",
            "        -9.3121e-02,  7.1905e-02, -1.0015e-02,  1.4092e-02, -1.1345e-01,\n",
            "         3.8180e-02,  1.0126e-02,  7.0310e-03,  5.3982e-02, -3.9536e-02,\n",
            "        -2.8165e-02,  5.4099e-02, -2.9172e-02, -3.5989e-02,  4.8562e-03,\n",
            "         7.3854e-02, -5.5994e-03, -4.2902e-02,  1.0728e-01, -5.9287e-02,\n",
            "        -3.4566e-02,  3.2613e-02,  2.7603e-02, -5.6044e-02,  1.9032e-02,\n",
            "        -4.9032e-02,  8.9472e-02,  8.0755e-02,  9.5729e-03,  8.6628e-02,\n",
            "         3.5794e-02, -8.8583e-02,  6.5718e-02, -1.3497e-01, -6.2858e-02,\n",
            "         5.3464e-02, -7.2231e-02, -5.8945e-03, -4.2553e-02, -2.7570e-03,\n",
            "        -4.5685e-02, -5.4946e-02,  6.7007e-02,  7.2671e-02, -2.7929e-02,\n",
            "        -3.2719e-02,  1.6821e-02,  8.1186e-02,  7.6381e-03, -4.4138e-02,\n",
            "         1.8443e-02,  2.0128e-02,  4.8938e-02,  4.2287e-02,  1.1477e-02,\n",
            "         3.5092e-02, -4.4755e-03, -3.8273e-02, -1.3313e-02,  2.0462e-02,\n",
            "        -1.9416e-02,  9.3235e-03, -6.5536e-02,  6.2253e-02, -6.8998e-02,\n",
            "         2.7305e-03, -1.9486e-02, -1.0548e-01,  5.1528e-02, -1.5250e-02,\n",
            "        -7.1785e-02, -7.1720e-02,  7.8891e-02, -6.0004e-02,  5.6809e-02,\n",
            "         3.0048e-02, -4.6062e-02,  9.9029e-02,  3.3288e-02,  6.7197e-02,\n",
            "        -7.2828e-02,  4.6600e-02, -8.7811e-03, -1.5861e-02,  3.5635e-02,\n",
            "         6.0559e-02, -6.1025e-02,  1.0089e-02,  7.3044e-02,  1.0580e-01,\n",
            "         4.5649e-02, -1.0443e-01, -5.1947e-02, -7.3827e-02, -7.1325e-03,\n",
            "         6.1841e-02,  1.8327e-02,  3.9981e-02, -2.1029e-02,  7.1751e-02,\n",
            "        -2.1121e-02,  6.7492e-03,  8.2168e-02, -1.2970e-02, -1.2338e-01,\n",
            "         3.7425e-02, -7.0744e-02,  1.7915e-02, -1.7410e-02,  4.0511e-02,\n",
            "        -2.1551e-02, -1.5696e-03, -6.7222e-03, -7.5162e-02, -3.5293e-02,\n",
            "         4.4373e-02,  1.2671e-03, -2.2459e-02,  3.4768e-03,  1.5244e-02,\n",
            "        -1.3515e-01, -3.5211e-02, -1.1570e-01, -3.7596e-04, -6.5250e-02,\n",
            "        -1.5762e-02, -3.4341e-02, -2.6148e-02,  6.2926e-02,  4.7941e-04,\n",
            "         6.6206e-02,  4.0069e-03, -3.1133e-02, -5.5699e-02,  1.2671e-02,\n",
            "         6.1750e-02, -1.0843e-02, -3.2952e-02, -7.6187e-02, -5.4012e-02,\n",
            "         6.1785e-02, -7.4979e-02, -1.0871e-01,  7.4933e-02,  2.1944e-02,\n",
            "        -2.3234e-02, -2.8797e-02,  9.1630e-02,  1.2871e-03, -1.6905e-02,\n",
            "        -1.0990e-07, -3.3136e-02,  1.5633e-02,  4.9208e-02, -3.6088e-03,\n",
            "         3.5210e-02,  7.5819e-02, -2.4442e-02, -4.9421e-03,  2.7072e-02,\n",
            "         1.1583e-01, -1.1037e-02,  1.8120e-02,  8.2696e-04, -2.7101e-02,\n",
            "         4.5659e-03,  5.1757e-02,  2.5026e-02,  1.7050e-02, -1.1584e-02,\n",
            "         2.4786e-02, -3.2067e-03, -2.4616e-02,  6.1570e-02,  3.5986e-02,\n",
            "         7.6110e-02,  5.4394e-02, -2.3698e-02,  4.6688e-03, -3.3483e-02,\n",
            "         3.1407e-02, -6.5772e-03,  7.7500e-02, -4.4905e-02,  6.1231e-02,\n",
            "        -5.5516e-02, -4.6928e-02, -7.1988e-03, -9.5299e-03, -3.7813e-02,\n",
            "         3.7395e-02,  8.5845e-02,  3.7815e-02,  5.8721e-02, -2.8430e-02,\n",
            "        -1.5847e-02, -5.8225e-03, -8.9312e-02,  2.2072e-02, -1.1574e-03,\n",
            "        -8.9343e-03,  8.6271e-02, -3.2985e-02,  5.0884e-02, -2.8095e-02,\n",
            "         8.4106e-02, -2.1976e-02,  4.9109e-03,  7.1879e-02,  2.1847e-02,\n",
            "        -3.6222e-02, -2.3718e-02, -1.7017e-01, -3.3177e-02,  5.0774e-02,\n",
            "         3.9547e-02,  3.1872e-02, -3.4370e-02,  6.3926e-02, -1.7733e-03,\n",
            "         5.8020e-03, -3.8901e-02,  6.0397e-02,  3.0917e-03,  1.9353e-02,\n",
            "         2.5921e-02,  8.1919e-02, -4.3882e-02, -6.0260e-03,  1.5639e-02,\n",
            "        -3.6363e-03,  1.0674e-01,  5.0382e-02, -6.5570e-02, -7.8117e-02,\n",
            "         6.3787e-02, -3.4645e-02,  6.5591e-02, -6.7388e-02,  2.6275e-02,\n",
            "        -5.8749e-02,  1.7788e-03, -1.8232e-02,  3.4314e-02, -5.8149e-02,\n",
            "        -1.9698e-03,  6.4525e-02,  1.1435e-02, -3.7726e-02,  8.9744e-02,\n",
            "         2.1157e-02,  4.8570e-02, -2.4858e-02, -3.8666e-02, -6.9865e-03,\n",
            "        -5.9044e-02,  4.6269e-02,  1.4461e-02,  5.8231e-02,  4.8242e-02,\n",
            "        -7.4787e-02, -3.6964e-02, -6.2413e-03, -1.1115e-02,  6.4276e-02,\n",
            "        -8.3089e-02, -7.0337e-02, -1.4084e-01, -4.4105e-02, -3.4628e-02,\n",
            "        -7.1736e-02,  4.7011e-02,  1.2530e-02, -1.0411e-01,  5.1197e-03,\n",
            "         1.0155e-02, -6.7232e-02, -5.3276e-02, -7.2471e-02, -7.4767e-02,\n",
            "         8.2437e-02,  3.3441e-02, -2.0278e-02, -1.6097e-02, -3.9139e-02,\n",
            "         3.9106e-02, -1.0144e-01, -1.0292e-01, -4.3737e-02,  6.6697e-02,\n",
            "        -1.4632e-02,  6.0055e-02, -5.8097e-02, -8.9765e-03, -1.6978e-02,\n",
            "        -1.7173e-01,  7.0303e-02, -3.5061e-02,  1.0067e-02, -8.1328e-02,\n",
            "         4.9080e-03, -2.5940e-02,  1.0475e-02, -7.1174e-02, -6.8884e-02,\n",
            "        -3.3217e-02, -2.2830e-02,  4.4601e-02,  7.6059e-02,  2.2985e-02,\n",
            "         2.5156e-04, -1.2451e-02,  2.2345e-02, -1.3061e-01, -6.8851e-03,\n",
            "        -3.0397e-02,  3.3075e-02,  1.0000e-01,  8.1095e-02,  8.4936e-02,\n",
            "        -4.0033e-02, -1.8232e-02, -2.2762e-02, -9.6584e-02,  2.0701e-02,\n",
            "        -3.0202e-02, -2.5917e-02, -1.5505e-02, -1.3717e-02, -4.3676e-02,\n",
            "         6.5913e-02,  2.5839e-02, -1.3746e-01, -6.4614e-02, -4.3073e-02,\n",
            "         1.6271e-01, -7.1331e-02,  8.8556e-02,  4.1210e-02, -4.1469e-02,\n",
            "         9.3741e-03, -8.4497e-03,  6.5181e-03,  4.4122e-02, -2.1770e-02,\n",
            "        -2.3095e-02, -2.5115e-03,  2.4408e-02, -8.5679e-03,  5.1431e-02,\n",
            "         4.6974e-02, -1.2071e-02,  3.4289e-02,  2.6185e-02,  1.0420e-02,\n",
            "         8.5933e-02, -2.0528e-02, -5.1591e-02, -1.1366e-03, -2.3772e-02,\n",
            "        -3.3174e-02,  2.2343e-02,  4.4505e-02, -1.1321e-02,  9.3009e-02,\n",
            "         6.0397e-02, -5.0706e-02,  1.2963e-02, -6.3809e-02,  3.0553e-02,\n",
            "         6.5404e-02,  4.9588e-02,  3.5326e-02, -5.0389e-02, -2.9662e-02,\n",
            "         2.0854e-02, -3.9160e-02, -1.4686e-02,  1.7375e-02, -4.1582e-02,\n",
            "         9.9740e-03, -4.5166e-02, -8.7071e-03,  2.8736e-02, -2.4667e-02,\n",
            "         3.0774e-02, -7.3968e-02,  1.1867e-01,  1.8083e-02, -4.5986e-02,\n",
            "        -3.9662e-02,  6.4616e-02, -3.0587e-02,  4.6108e-02,  9.1946e-02,\n",
            "         3.3129e-02, -7.3844e-02,  4.7138e-02, -5.7931e-03, -5.7479e-03,\n",
            "        -1.9616e-02,  1.1179e-02,  1.6343e-02,  2.1510e-02, -7.3451e-02,\n",
            "        -4.0027e-02,  9.3050e-02, -9.7006e-02, -2.3464e-02, -8.4274e-02,\n",
            "         2.2720e-02,  1.0088e-01,  5.6101e-02,  3.3015e-02,  5.9631e-02,\n",
            "         1.9544e-02,  8.8202e-02,  3.3407e-02,  2.8507e-02, -2.1061e-02,\n",
            "        -8.1278e-02,  4.9280e-02, -1.0292e-01,  1.2509e-02, -2.3340e-02,\n",
            "        -6.1037e-02,  5.3545e-02, -3.9393e-03, -5.3998e-02,  1.0433e-02,\n",
            "        -7.3436e-02,  5.4192e-02, -3.2485e-02, -3.9576e-02, -1.0595e-02,\n",
            "        -4.8318e-02,  7.7092e-03, -7.6964e-02,  3.7133e-02, -1.0358e-02,\n",
            "         8.2689e-02,  3.9626e-03, -2.3198e-02,  2.2095e-02,  1.2224e-03,\n",
            "        -9.1519e-02,  4.2796e-02,  1.7901e-01, -4.3019e-02, -1.7391e-02,\n",
            "         1.4155e-02, -1.1567e-02, -2.7114e-02, -1.7543e-02,  1.6933e-02,\n",
            "        -1.6846e-02, -5.3713e-02,  7.9380e-02,  4.4082e-02, -8.5949e-03,\n",
            "         6.5421e-02, -5.8373e-02,  1.6302e-02, -1.4995e-01, -2.3830e-03,\n",
            "        -7.9517e-02, -7.5184e-02,  7.2936e-02, -5.7392e-02,  4.0702e-02,\n",
            "         2.6742e-02,  3.7695e-02,  1.3776e-01,  4.0800e-02, -2.5246e-02,\n",
            "        -4.3337e-02,  1.2784e-02,  1.8766e-02, -2.0654e-02, -5.7589e-02,\n",
            "        -1.2899e-02, -6.3061e-02,  4.9715e-02, -7.1740e-02, -4.9187e-03,\n",
            "        -7.9432e-02, -7.4660e-02, -4.0668e-02, -7.8446e-02,  5.9205e-02,\n",
            "         9.7038e-04,  8.5833e-02,  1.2807e-02,  3.6577e-02,  3.6192e-03,\n",
            "         5.0693e-03, -3.4755e-02,  1.1447e-02,  1.3317e-01, -7.7685e-02,\n",
            "        -3.3224e-02,  2.2473e-02,  4.7617e-03, -6.3294e-02, -4.0824e-02,\n",
            "         9.3818e-03, -3.5482e-02, -1.8887e-03, -1.3443e-03, -1.3563e-02,\n",
            "        -7.4471e-03,  3.4053e-02,  4.2373e-02, -9.9705e-02,  2.0403e-02,\n",
            "         4.9540e-04,  5.2281e-02,  6.6492e-02,  3.9157e-02, -1.1098e-01,\n",
            "         4.8791e-02,  3.1685e-02, -3.1455e-02,  3.5341e-02,  6.5292e-03,\n",
            "         2.8326e-03, -3.8901e-02,  2.7648e-03, -7.2907e-03, -1.2871e-02,\n",
            "        -3.1784e-03,  3.2951e-02,  1.6476e-02,  1.6282e-02,  1.6378e-02,\n",
            "        -8.4773e-03,  6.0504e-02, -2.5894e-02, -2.8988e-02,  3.2153e-02,\n",
            "        -2.4028e-02,  6.3870e-02, -6.4780e-02,  4.3593e-02, -4.2202e-02,\n",
            "         3.4633e-02,  3.7958e-02, -1.2637e-02,  4.4681e-02,  4.1626e-02,\n",
            "        -3.5623e-03, -5.6816e-02,  2.1152e-02, -6.7450e-02,  2.3761e-02,\n",
            "         8.1095e-02, -5.1225e-02, -1.7370e-01, -5.4643e-02,  5.3265e-02,\n",
            "        -8.3569e-03, -2.4397e-02,  8.7371e-02, -5.6417e-02,  5.0921e-03,\n",
            "         4.6526e-03,  1.2270e-01, -3.8925e-02, -1.9897e-02,  1.0261e-02,\n",
            "         4.7916e-02, -6.4644e-03, -1.3603e-02, -6.0670e-02, -1.8599e-02,\n",
            "         1.0948e-03, -5.5647e-02,  5.7013e-02, -6.1004e-02, -1.5668e-02,\n",
            "        -4.6523e-02,  4.3614e-02, -8.7036e-02, -5.4412e-02, -4.7350e-02,\n",
            "         1.2413e-01, -3.8469e-02, -6.6529e-02,  1.6275e-02,  6.0928e-02,\n",
            "        -2.4207e-02, -1.9780e-02, -9.0726e-02, -4.0924e-02,  1.0616e-02,\n",
            "        -6.8781e-02,  3.4088e-03,  8.8122e-02,  5.1344e-03,  3.4482e-02,\n",
            "        -1.1410e-02, -3.8252e-02, -2.2138e-03, -4.2809e-03,  6.1339e-02,\n",
            "         6.7007e-02,  3.7613e-02,  5.8017e-02,  5.5622e-02, -8.2084e-03,\n",
            "         4.5584e-02,  9.8808e-02, -1.4492e-02, -4.0127e-02, -6.1635e-02,\n",
            "         5.1287e-02, -1.1582e-01,  2.1431e-02, -2.5194e-02, -2.0637e-02,\n",
            "        -6.5014e-02, -2.9049e-02, -2.3611e-02,  2.9850e-02,  2.9458e-02,\n",
            "         9.3310e-03,  5.3541e-02, -2.6162e-02,  1.4447e-02, -6.9495e-03,\n",
            "        -9.4771e-03, -8.3426e-03, -5.1948e-04,  1.3216e-01, -7.4474e-02,\n",
            "         4.7550e-02, -2.7902e-02, -2.3927e-02,  5.5855e-03, -1.2512e-01,\n",
            "         1.2321e-03, -3.2836e-02,  5.9448e-02,  1.0768e-01,  1.9324e-02,\n",
            "        -6.3382e-02, -7.0183e-02,  4.2119e-03,  9.4455e-02, -1.1350e-01,\n",
            "         1.9106e-02, -3.0546e-02, -4.5780e-03,  1.7279e-02,  8.4914e-02,\n",
            "         9.0435e-03,  2.7378e-02,  1.6080e-02,  4.5765e-02,  8.3957e-02,\n",
            "         4.9190e-02, -1.9775e-02,  2.2739e-02,  5.2321e-02,  1.6538e-02,\n",
            "        -3.4811e-02,  7.4847e-03,  2.4267e-02, -2.2607e-02,  5.0228e-02,\n",
            "        -4.7741e-02,  3.9192e-02, -1.3379e-01,  9.5285e-02, -6.1002e-02,\n",
            "        -2.1624e-02,  6.2862e-03,  1.4943e-02, -2.7696e-03,  3.2522e-02,\n",
            "        -3.9330e-02, -4.8936e-02, -2.9460e-02, -5.9667e-02,  9.2256e-03,\n",
            "        -3.1645e-02, -1.9984e-02,  1.0579e-02,  6.1772e-02, -1.2866e-02,\n",
            "         8.8945e-02,  1.1847e-02,  5.2178e-02,  2.3542e-03,  7.9107e-02,\n",
            "        -2.4286e-02,  3.7648e-02, -8.7268e-02,  6.4735e-03, -1.5933e-02,\n",
            "         5.4524e-02, -2.2718e-02, -1.5706e-02, -1.3687e-02,  1.9453e-02,\n",
            "         5.6279e-03,  1.2697e-03, -9.2853e-02,  4.4273e-02, -5.1991e-03,\n",
            "        -8.9988e-02, -1.2943e-01,  7.9055e-02, -2.1045e-02, -4.0911e-03,\n",
            "         7.5035e-03,  1.3402e-02, -2.6238e-02,  9.8201e-03, -2.0730e-02,\n",
            "        -9.4597e-02, -1.5724e-02,  3.5030e-02,  3.0882e-02, -1.6926e-02,\n",
            "         2.0355e-02,  4.4147e-02,  5.7260e-02, -8.1001e-02, -3.8487e-02,\n",
            "         9.3078e-02,  4.7728e-03, -4.6130e-02, -3.6741e-02, -1.5273e-02,\n",
            "        -3.0716e-03,  4.8581e-02, -8.1718e-02, -3.8994e-02,  2.3976e-02,\n",
            "         1.1522e-01,  3.8608e-02, -1.2535e-04, -5.6584e-02,  4.7605e-02,\n",
            "         3.0147e-02, -2.8815e-02, -3.7909e-02,  3.1418e-02,  6.6714e-02,\n",
            "        -2.1056e-02,  2.7724e-02,  1.0103e-01,  9.9266e-02, -7.5616e-02,\n",
            "        -4.1905e-03, -1.0415e-02,  1.1259e-01,  1.1641e-02,  3.2574e-02,\n",
            "         1.0299e-01,  3.6907e-02, -3.7690e-02,  7.3003e-02, -7.2201e-02,\n",
            "        -5.5779e-02,  1.0074e-03, -9.5599e-03, -2.4459e-02,  1.8065e-02,\n",
            "        -7.9670e-02,  5.1998e-02, -3.2187e-02,  5.5949e-02, -1.0501e-01,\n",
            "        -1.2751e-02, -3.3769e-02, -1.8276e-02,  2.7946e-02, -1.0274e-01,\n",
            "         1.3451e-02,  1.4802e-03, -2.0966e-02,  4.9948e-02,  1.3822e-01,\n",
            "         6.8914e-02, -1.9070e-04, -2.3758e-02,  5.8064e-02, -2.5884e-02,\n",
            "        -4.3999e-02, -1.7285e-01,  8.5945e-02,  7.3891e-03,  1.0532e-02,\n",
            "        -4.6523e-03, -9.9775e-02, -1.7112e-02,  2.3252e-03, -1.4203e-02,\n",
            "        -5.7218e-03,  4.6643e-02,  2.1072e-03, -2.7243e-02,  3.7232e-02,\n",
            "         4.9328e-02,  2.6738e-02,  1.1906e-01, -1.5301e-02, -5.3008e-02,\n",
            "         3.2408e-02, -2.4123e-02, -1.3259e-01, -9.9767e-03, -1.3086e-03,\n",
            "         2.9709e-02,  8.0993e-02, -4.1711e-02,  5.8645e-02,  4.9483e-02])\n",
            "tensor([0.0010, 0.0010, 0.0009, 0.0010, 0.0010, 0.0011, 0.0011, 0.0009, 0.0009,\n",
            "        0.0010, 0.0009, 0.0010, 0.0010, 0.0010, 0.0010, 0.0009, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0011, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0011, 0.0009, 0.0010, 0.0011, 0.0010,\n",
            "        0.0009, 0.0010, 0.0009, 0.0010, 0.0010, 0.0010, 0.0010, 0.0009, 0.0009,\n",
            "        0.0011, 0.0009, 0.0010, 0.0009, 0.0010, 0.0011, 0.0010, 0.0010, 0.0010,\n",
            "        0.0011, 0.0010, 0.0010, 0.0010, 0.0009, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0009, 0.0010, 0.0011, 0.0010, 0.0010, 0.0010, 0.0010, 0.0009,\n",
            "        0.0011, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0011, 0.0011, 0.0010, 0.0010, 0.0010, 0.0009, 0.0010, 0.0009,\n",
            "        0.0009, 0.0011, 0.0010, 0.0010, 0.0010, 0.0010, 0.0009, 0.0011, 0.0010,\n",
            "        0.0011, 0.0010, 0.0010, 0.0010, 0.0009, 0.0010, 0.0010, 0.0011, 0.0009,\n",
            "        0.0010, 0.0010, 0.0009, 0.0011, 0.0010, 0.0010, 0.0010, 0.0010, 0.0009,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0009, 0.0011, 0.0010,\n",
            "        0.0010, 0.0011, 0.0010, 0.0009, 0.0010, 0.0011, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0011, 0.0010, 0.0010, 0.0009, 0.0011, 0.0009, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0009, 0.0011, 0.0010, 0.0009, 0.0010, 0.0010,\n",
            "        0.0009, 0.0009, 0.0009, 0.0009, 0.0010, 0.0011, 0.0009, 0.0010, 0.0010,\n",
            "        0.0009, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0009, 0.0011, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "        0.0009, 0.0011, 0.0010, 0.0010, 0.0009, 0.0010, 0.0010, 0.0010, 0.0011,\n",
            "        0.0010, 0.0010, 0.0011, 0.0010, 0.0010, 0.0010, 0.0011, 0.0010, 0.0010,\n",
            "        0.0011, 0.0009, 0.0010, 0.0010, 0.0010, 0.0009, 0.0010, 0.0010, 0.0011,\n",
            "        0.0011, 0.0010, 0.0011, 0.0010, 0.0009, 0.0011, 0.0009, 0.0009, 0.0011,\n",
            "        0.0009, 0.0010, 0.0010, 0.0010, 0.0010, 0.0009, 0.0011, 0.0011, 0.0010,\n",
            "        0.0010, 0.0010, 0.0011, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0009,\n",
            "        0.0011, 0.0009, 0.0010, 0.0010, 0.0009, 0.0011, 0.0010, 0.0009, 0.0009,\n",
            "        0.0011, 0.0009, 0.0011, 0.0010, 0.0010, 0.0011, 0.0010, 0.0011, 0.0009,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0011, 0.0009, 0.0010, 0.0011, 0.0011,\n",
            "        0.0010, 0.0009, 0.0009, 0.0009, 0.0010, 0.0011, 0.0010, 0.0010, 0.0010,\n",
            "        0.0011, 0.0010, 0.0010, 0.0011, 0.0010, 0.0009, 0.0010, 0.0009, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0009, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0009, 0.0010, 0.0009, 0.0010, 0.0009, 0.0010,\n",
            "        0.0010, 0.0010, 0.0011, 0.0010, 0.0011, 0.0010, 0.0010, 0.0009, 0.0010,\n",
            "        0.0011, 0.0010, 0.0010, 0.0009, 0.0009, 0.0011, 0.0009, 0.0009, 0.0011,\n",
            "        0.0010, 0.0010, 0.0010, 0.0011, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "        0.0011, 0.0010, 0.0010, 0.0011, 0.0010, 0.0010, 0.0010, 0.0011, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0011, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0011, 0.0010, 0.0011, 0.0011, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0011, 0.0010, 0.0011, 0.0009, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0011, 0.0010, 0.0011, 0.0010, 0.0010, 0.0010, 0.0009,\n",
            "        0.0010, 0.0010, 0.0010, 0.0011, 0.0010, 0.0011, 0.0010, 0.0011, 0.0010,\n",
            "        0.0010, 0.0011, 0.0010, 0.0010, 0.0010, 0.0008, 0.0010, 0.0011, 0.0010,\n",
            "        0.0010, 0.0010, 0.0011, 0.0010, 0.0010, 0.0010, 0.0011, 0.0010, 0.0010,\n",
            "        0.0010, 0.0011, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011, 0.0011, 0.0009,\n",
            "        0.0009, 0.0011, 0.0010, 0.0011, 0.0009, 0.0010, 0.0009, 0.0010, 0.0010,\n",
            "        0.0010, 0.0009, 0.0010, 0.0011, 0.0010, 0.0010, 0.0011, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0009, 0.0010, 0.0010, 0.0011, 0.0010, 0.0009,\n",
            "        0.0010, 0.0010, 0.0010, 0.0011, 0.0009, 0.0009, 0.0009, 0.0010, 0.0010,\n",
            "        0.0009, 0.0010, 0.0010, 0.0009, 0.0010, 0.0010, 0.0009, 0.0009, 0.0009,\n",
            "        0.0009, 0.0011, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0009, 0.0009,\n",
            "        0.0010, 0.0011, 0.0010, 0.0011, 0.0009, 0.0010, 0.0010, 0.0008, 0.0011,\n",
            "        0.0010, 0.0010, 0.0009, 0.0010, 0.0010, 0.0010, 0.0009, 0.0009, 0.0010,\n",
            "        0.0010, 0.0010, 0.0011, 0.0010, 0.0010, 0.0010, 0.0010, 0.0009, 0.0010,\n",
            "        0.0010, 0.0010, 0.0011, 0.0011, 0.0011, 0.0010, 0.0010, 0.0010, 0.0009,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011, 0.0010, 0.0009,\n",
            "        0.0009, 0.0010, 0.0012, 0.0009, 0.0011, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0011, 0.0010, 0.0009, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0011, 0.0011, 0.0010, 0.0010, 0.0009,\n",
            "        0.0010, 0.0011, 0.0011, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0009,\n",
            "        0.0011, 0.0010, 0.0010, 0.0010, 0.0011, 0.0010, 0.0010, 0.0011, 0.0010,\n",
            "        0.0009, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0009,\n",
            "        0.0010, 0.0011, 0.0009, 0.0010, 0.0009, 0.0010, 0.0011, 0.0011, 0.0010,\n",
            "        0.0011, 0.0010, 0.0011, 0.0010, 0.0010, 0.0010, 0.0009, 0.0011, 0.0009,\n",
            "        0.0010, 0.0010, 0.0009, 0.0011, 0.0010, 0.0009, 0.0010, 0.0009, 0.0011,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0009, 0.0010, 0.0010, 0.0011,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0009, 0.0010, 0.0012, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0009, 0.0011, 0.0010,\n",
            "        0.0010, 0.0011, 0.0009, 0.0010, 0.0009, 0.0010, 0.0009, 0.0009, 0.0011,\n",
            "        0.0009, 0.0010, 0.0010, 0.0010, 0.0011, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0009, 0.0010, 0.0009, 0.0011, 0.0009, 0.0010, 0.0009,\n",
            "        0.0009, 0.0010, 0.0009, 0.0011, 0.0010, 0.0011, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0011, 0.0009, 0.0010, 0.0010, 0.0010, 0.0009,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "        0.0009, 0.0010, 0.0010, 0.0011, 0.0011, 0.0010, 0.0009, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0011, 0.0009, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0009, 0.0010, 0.0009, 0.0010, 0.0011, 0.0009, 0.0008,\n",
            "        0.0009, 0.0011, 0.0010, 0.0010, 0.0011, 0.0009, 0.0010, 0.0010, 0.0011,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0009, 0.0010, 0.0010,\n",
            "        0.0009, 0.0011, 0.0009, 0.0010, 0.0010, 0.0010, 0.0009, 0.0009, 0.0010,\n",
            "        0.0011, 0.0010, 0.0009, 0.0010, 0.0011, 0.0010, 0.0010, 0.0009, 0.0010,\n",
            "        0.0010, 0.0009, 0.0010, 0.0011, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0011, 0.0011, 0.0010, 0.0011, 0.0011, 0.0010, 0.0010, 0.0011,\n",
            "        0.0010, 0.0010, 0.0009, 0.0011, 0.0009, 0.0010, 0.0010, 0.0010, 0.0009,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0011, 0.0009, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "        0.0009, 0.0010, 0.0010, 0.0011, 0.0011, 0.0010, 0.0009, 0.0009, 0.0010,\n",
            "        0.0011, 0.0009, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0011, 0.0011, 0.0010, 0.0010, 0.0011, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0011, 0.0010, 0.0010, 0.0009, 0.0011, 0.0009,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0009,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0011, 0.0010, 0.0011, 0.0010, 0.0011,\n",
            "        0.0010, 0.0011, 0.0010, 0.0010, 0.0009, 0.0010, 0.0010, 0.0011, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0009, 0.0010, 0.0010, 0.0009,\n",
            "        0.0009, 0.0011, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "        0.0009, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0011, 0.0009,\n",
            "        0.0010, 0.0011, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0009,\n",
            "        0.0010, 0.0010, 0.0011, 0.0010, 0.0010, 0.0009, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0011, 0.0010, 0.0010, 0.0011, 0.0011, 0.0009, 0.0010,\n",
            "        0.0010, 0.0011, 0.0010, 0.0010, 0.0011, 0.0010, 0.0010, 0.0011, 0.0009,\n",
            "        0.0009, 0.0010, 0.0010, 0.0010, 0.0010, 0.0009, 0.0011, 0.0010, 0.0011,\n",
            "        0.0009, 0.0010, 0.0010, 0.0010, 0.0010, 0.0009, 0.0010, 0.0010, 0.0010,\n",
            "        0.0011, 0.0011, 0.0011, 0.0010, 0.0010, 0.0011, 0.0010, 0.0010, 0.0008,\n",
            "        0.0011, 0.0010, 0.0010, 0.0010, 0.0009, 0.0010, 0.0010, 0.0010, 0.0010,\n",
            "        0.0010, 0.0010, 0.0010, 0.0010, 0.0011, 0.0010, 0.0011, 0.0010, 0.0009,\n",
            "        0.0010, 0.0010, 0.0009, 0.0010, 0.0010, 0.0010, 0.0011, 0.0010, 0.0011,\n",
            "        0.0011])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download ImageNet labels\n",
        "!wget https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c8jXHTMN3cW",
        "outputId": "ee7cb33e-7f7f-4e20-e8b0-70cf8175f922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-07 08:38:25--  https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10472 (10K) [text/plain]\n",
            "Saving to: ‘imagenet_classes.txt.1’\n",
            "\n",
            "\rimagenet_classes.tx   0%[                    ]       0  --.-KB/s               \rimagenet_classes.tx 100%[===================>]  10.23K  --.-KB/s    in 0s      \n",
            "\n",
            "2022-11-07 08:38:25 (66.9 MB/s) - ‘imagenet_classes.txt.1’ saved [10472/10472]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the categories\n",
        "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
        "    categories = [s.strip() for s in f.readlines()]\n",
        "\n",
        "# Show top categories per image\n",
        "top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
        "\n",
        "for i in range(top5_prob.size(0)):\n",
        "    print(categories[top5_catid[i]], top5_prob[i].item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUjR1kafN64f",
        "outputId": "9a30d7f8-0419-41b3-af69-86ecc9524202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "limousine 0.001195721561089158\n",
            "cowboy hat 0.0011763875372707844\n",
            "potpie 0.001147919800132513\n",
            "military uniform 0.0011473956983536482\n",
            "neck brace 0.0011421365197747946\n"
          ]
        }
      ]
    }
  ]
}